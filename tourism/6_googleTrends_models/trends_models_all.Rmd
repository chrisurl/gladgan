---
title: "Google trends models"
author: Christian Url
date: 'Last Compiled `r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document:
    
    code_folding: show
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
               collapse = FALSE,
               comment = "",
               strip.white = TRUE,
               warning = FALSE,
               message = FALSE,
               cache = TRUE,
               out.width = "100%",
               fig.align = "center")

```

## load Packages
```{r, cache=FALSE}
library(data.table)
library(lubridate)
library(caret)
library(rai)
library(forecast)
library(parallel)
library(doParallel)
library(tidyverse)
```

## Paths
```{r}
datapath = "../data/"
trends_path = paste0(datapath, "googleTrendsAuto/")
ind_path = paste0(datapath, "indicators/")
```

## Functions
```{r}
scale2 <- function(x, na.rm = TRUE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)
```


# Read data {.tabset .tabset-fade}
## Volatility Index {-}
```{r vol_ind}
vol_ind = read_delim(paste0(datapath,"country_volatility_index.csv")) %>%
  mutate(geo = str_split(Countries, " ", simplify = T)[,1], .before = 1,
         country = str_remove(Countries, ".*\\("), 
         country = str_extract(country, ".*(?=\\))"))

country_code_list = unlist(vol_ind$geo)
country_list = unlist(vol_ind$country)
#country_code_list[country_list=="Greece"] <- "GR"
```

## Indicators {-}
Read indicator data and filter for valid countries. Countries are valid if they appear in the country volatility index.

```{r ind1}
list.files(ind_path)
ind_in = fread(paste0(ind_path,"tour_occ_nim_linear_all.csv"))

ind_t1 = ind_in[,-c(1:3)] %>% 
  as_tibble() %>%
  mutate(month_end = as_date(paste0(TIME_PERIOD,"-01")),.before=1) %>%
  filter(geo %in% vol_ind$geo)

ind_t1 %>% select(nace_r2) %>% distinct()
```

Build dataset:
```{r ind2}
df_ind = ind_t1 %>%
  filter(nace_r2 == "I551-I553" & unit=="NR") %>%
  group_by(geo, c_resid) %>%
  mutate(obs_value_std = scale2(OBS_VALUE), mean=mean(OBS_VALUE), sd = sd(OBS_VALUE)) %>%
  ungroup()

ind_wide = ind_t1 %>%
  filter(nace_r2 == "I551-I553" & unit=="NR") %>%
  mutate(obs_value_std = OBS_VALUE) %>%
  pivot_wider(id_cols = c(month_end, geo), names_from=c_resid, values_from = obs_value_std, names_prefix="resid_") %>%
  group_by(geo) %>%
  mutate(resid_DOM = coalesce(resid_DOM, resid_NAT),
         share_FOR = resid_FOR / resid_TOTAL,
         month = month(month_end),
         year = year(month_end),
         resid_DOM_STD = scale2(resid_DOM),
         resid_FOR_STD = scale2(resid_FOR),
         resid_TOTAL_STD = scale2(resid_TOTAL),
         ) %>%
  select(!resid_NAT) %>%
  ungroup() %>%
  arrange(geo,month_end)

ind_wide %>%
  filter(is.na(resid_DOM)) %>%
  distinct(geo)

ind_stats = ind_wide %>%
  group_by(geo) %>%
  summarise(resid_TOTAL_mean=mean(resid_TOTAL, na.rm = T), 
            resid_TOTAL_sd=sd(resid_TOTAL, na.rm = T), 
            resid_TOTAL_max=max(resid_TOTAL, na.rm = T), 
            resid_TOTAL_norm=max(abs(resid_TOTAL-mean(resid_TOTAL, na.rm = T)), na.rm=T)
            )

geo_minmax = ind_wide %>%
  group_by(geo) %>%
  summarise(min = min(year), max = max(year))

```

## Google Trends {-}
```{r trends}
list.files(trends_path)
goo = read_csv(paste0(trends_path, "trends_full.csv"))
trends_100 = goo %>%
  mutate(across(3:13, ~.x/100))
trends_norm = goo %>%
  mutate(across(3:13, scale2))
```

## Combine data {-}
```{r combine}
trends = ind_wide %>%
  select(month_end, geo, resid_TOTAL_STD) %>%
  rename(date = month_end, country_code = geo) %>%
  inner_join(trends_100, by=c("date", "country_code"))
```

# Plots {.tabset .tabset-fade}
```{r}
colnames(trends)
```

## country_trav {-}
```{r plot1}
trends %>%
  ggplot(mapping=aes(x=date, y=country_trav, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```


## country_cc_trav {-}
```{r plot2}
trends %>%
  ggplot(mapping=aes(x=date, y=country_cc_trav, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```

## country_hotels {-}
```{r plot3}
trends %>%
  ggplot(mapping=aes(x=date, y=country_hotels, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```

## cc_restaurant {-}
```{r plot4}
trends %>%
  ggplot(mapping=aes(x=date, y=cc_restaurant, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```

## cc_fooddrink {-}
```{r plot5}
trends %>%
  ggplot(mapping=aes(x=date, y=cc_fooddrink, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```

# Modelling for all countries {.tabset .tabset-fade}

```{r model data, eval = F}

cl = detectCores()-1
registerDoParallel(cl)

out_list = list()

out_list = foreach(i = 1:length(country_code_list), .packages=c("tidyverse","forecast", "rai", "caret")) %dopar% {
  
  print(paste0("Country ", i,"/24"))
  
  c_code = country_code_list[i]
  ctry = country_list[i]
  
  trends_cty = trends %>%
    filter(country_code == c_code) %>%
    zoo::na.locf() #carry last obs forward
  
  trends_ts = trends_cty %>%
    select(!c(country_code,date)) %>%
    ts(start=c(2004,1), frequency=12)
  
  theData = trends_cty %>%
    .[,-c(1:3)]
  
  theResponse = trends_cty %>% 
    .$resid_TOTAL_STD
  
  stats_list = list()
  
  mod_rai = rai(theData = theData, theResponse = theResponse)
  (rai_sum = summary(mod_rai$model))
  
  sig_sq = (rai_sum$sigma)^2
  
  stats_list[[1]] = data.frame(country = country_list[i],
                               TrainRMSE = RMSE(predict(mod_rai, theData),theResponse),
                               TrainRsquared  = rai_sum$r.squared,
                               TrainMAE = MAE(predict(mod_rai, theData),theResponse),
                               method = "RAI")
  
  X = as.matrix(theData)
  mod1 = auto.arima(y = theResponse, max.order = 12, xreg = X,  max.D = 2)
  ar_sum = summary(mod1)
  
  stats_list[[2]] = data.frame(country = country_list[i],
                               TrainRMSE = RMSE(ar_sum$fitted, ar_sum$x),
                               TrainRsquared  = NA,
                               TrainMAE = MAE(ar_sum$fitted, ar_sum$x),
                               method = "ARIMAX")
  
  fitControl <- caret::trainControl(method = "repeatedcv",
                                    number = 20, ## 20-fold CV..., only want to predict small amounts of data
                                    repeats = 10)  ## repeated 10 times
  mod_svn =caret::train(x=theData, y=theResponse, method="svmRadialSigma", trControl = fitControl, verbose=F)
  stats_list[[3]] = getTrainPerf(mod_svn) %>% 
    add_column(country = country_list[i], .before = 1)
  
  theData = as.data.frame(theData)
  
  gbmGrid <-  expand.grid(interaction.depth = c(1:4), 
                          n.trees = c(20,50,75,100, 150, 200,350,500), 
                          shrinkage = 0.1,
                          n.minobsinnode = 5)
  mod_gbm =caret::train(x=theData, y=theResponse, method="gbm", trControl = fitControl, tuneGrid=gbmGrid, verbose=F)
  
  stats_list[[4]] = getTrainPerf(mod_gbm) %>% 
    add_column(country = country_list[i], .before = 1)
  
  mod_rf =caret::train(x=theData, y=theResponse, method="rf", trControl = fitControl, verbose=F)
  stats_list[[5]] = getTrainPerf(mod_rf) %>% 
    add_column(country = country_list[i], .before = 1)
  
  mod_brnn =caret::train(x=theData, y=theResponse, method="brnn", trControl = fitControl, verbose=F)
  stats_list[[6]] = getTrainPerf(mod_brnn) %>% 
    add_column(country = country_list[i], .before = 1)
  
  xgb_trcontrol = trainControl(
    method = "cv",
    number = 5,  
    allowParallel = FALSE,
    verboseIter = FALSE,
    returnData = FALSE
  )
  
  xgbGrid <- expand.grid(nrounds = c(50,100,200,250),  # this is n_estimators in the python code above
                         max_depth = c(1:4, 10, 15, 20, 25),
                         colsample_bytree = seq(0.5, 0.9, length.out = 5),
                         ## The values below are default values in the sklearn-api. 
                         eta = 0.1,
                         gamma=0,
                         min_child_weight = 1,
                         subsample = 1
  )
  xgb_model = caret::train(x=theData, y=theResponse,  
    trControl = xgb_trcontrol,
    tuneGrid = xgbGrid,
    method = "xgbTree"
  )
  
  stats_list[[7]] = getTrainPerf(xgb_model) %>% 
    add_column(country = country_list[i], .before = 1)
  
  mod_svn2 =caret::train(x=theData, y=theResponse, method="svmPoly", trControl = fitControl, verbose=F)
  stats_list[[8]] = getTrainPerf(mod_svn2) %>% 
    add_column(country = country_list[i], .before = 1)
  
  
  mod_svn3 =caret::train(x=theData, y=theResponse, method="svmLinear3", trControl = fitControl, verbose=F)
  stats_list[[9]] = getTrainPerf(mod_svn3) %>% 
    add_column(country = country_list[i], .before = 1)
  
  
  res_df = do.call(rbind, stats_list)
}

stopImplicitCluster()
stats = do.call(bind_rows, out_list)

write_csv(stats, "modelling_stats_all.csv")
```



# Results for div100 regressors {.tabset .tabset-fade}

```{r}
knitr::opts_chunk$set(cache = FALSE)
```


```{r, cache=FALSE}
stats = readr::read_csv("stats_alltrends_100.csv")
a  = stats %>%
  filter(method == "ARIMAX")

print(a, n=nrow(a))
```


## Overview {-}
```{r, cache=FALSE}
print(stats, n=nrow(stats))
```

## Best fits RMSE {-}
```{r, cache=FALSE}
rmse = stats %>%
  filter(method != "ARIMAX") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            min_RMSE = min(TrainRMSE),
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRMSE==min_RMSE)
print(rmse, n=nrow(rmse))
```

## Best fits R2 {-}
```{r}
r2 = stats %>%
  filter(method != "ARIMAX") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRsquared == max_R2)

print(r2, n=nrow(r2))
```

## Best fits MAE {-}

```{r}
mae = stats %>%
  filter(method != "ARIMAX") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, TrainMAE,
            min_MAE = min(TrainMAE)) %>%
  filter(TrainMAE == min_MAE)
  
print(mae, n=nrow(mae))
```

# Results for normalised regressors {.tabset .tabset-fade}


```{r, cache=FALSE}
stats = readr::read_csv("stats_alltrends_norm.csv")
a  = stats %>%
  filter(method == "ARIMAX")

print(a, n=nrow(a))
```


## Overview {-}
```{r, cache=FALSE}
print(stats, n=nrow(stats))
```

## Best fits RMSE {-}
```{r, cache=FALSE}
rmse = stats %>%
  filter(method != "ARIMAX") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            min_RMSE = min(TrainRMSE),
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRMSE==min_RMSE)
print(rmse, n=nrow(rmse))
```

## Best fits R2 {-}
```{r}
r2 = stats %>%
  filter(method != "ARIMAX") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRsquared == max_R2)

print(r2, n=nrow(r2))
```

## Best fits MAE {-}

```{r}
mae = stats %>%
  filter(method != "ARIMAX") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, TrainMAE,
            min_MAE = min(TrainMAE)) %>%
  filter(TrainMAE == min_MAE)
  
print(mae, n=nrow(mae))
```


# Results for lagged div100 regressors {.tabset .tabset-fade}


```{r, cache=FALSE}
stats = readr::read_csv("stats_alltrends_100_lag.csv")
```


## Overview {-}
```{r, cache=FALSE}
print(stats, n=nrow(stats))
```

## RAI {-}
```{r}
a  = stats %>%
  filter(method == "RAI")

print(a, n=nrow(a))
```

## Best fits RMSE {-}
```{r, cache=FALSE}
rmse = stats %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            min_RMSE = min(TrainRMSE),
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRMSE==min_RMSE)
print(rmse, n=nrow(rmse))
```

## Best fits R2 {-}
```{r}
r2 = stats %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRsquared == max_R2)

print(r2, n=nrow(r2))
```

## Best fits MAE {-}

```{r}
mae = stats %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, TrainMAE,
            min_MAE = min(TrainMAE)) %>%
  filter(TrainMAE == min_MAE)
  
print(mae, n=nrow(mae))
```


# Results for lagged normalised regressors {.tabset .tabset-fade}


```{r, cache=FALSE}
stats = readr::read_csv("stats_alltrends_norm_lag.csv")
```


## Overview {-}
```{r, cache=FALSE}
print(stats, n=nrow(stats))
```

## RAI {-}
```{r}
a  = stats %>%
  filter(method == "RAI")

print(a, n=nrow(a))
```

## Best fits RMSE {-}
```{r, cache=FALSE}
rmse = stats %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            min_RMSE = min(TrainRMSE),
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRMSE==min_RMSE)
print(rmse, n=nrow(rmse))
```

## Best fits R2 {-}
```{r}
r2 = stats %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRsquared == max_R2)

print(r2, n=nrow(r2))
```

## Best fits MAE {-}

```{r}
mae = stats %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, TrainMAE,
            min_MAE = min(TrainMAE)) %>%
  filter(TrainMAE == min_MAE)
  
print(mae, n=nrow(mae))
```



# ARIMAX Modelling

__See extra file!__

Idea: Test variables in a stepwise procedure s.t. only the relevant terms are included in final model
Also use the hints discussed here: https://math.unm.edu/~lil/Stat581/10-dynamic-regression.pdf and https://www.r-bloggers.com/2021/10/dynamic-regression-with-arima-errors-the-students-on-the-streets/

Main Ideas: 

  * Try Fourier part with $K \in 1,2,...,6,12$
  * Trend, Drift, Season in extra terms or are they already included?
  * The *fable* package (https://fable.tidyverts.org/) should be an easy way to tidy up the estimation process 
  * Should we use the RAI Package to identify the regression part?


Readings: 

  * ARIMA Chapter: https://otexts.com/fpp2/arima.html
  * Dynamic regression: https://otexts.com/fpp2/dynamic.html
  * Neural network models: https://otexts.com/fpp2/nnetar.html
  