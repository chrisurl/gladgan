---
title: "Google trends models"
author: Christian Url
date: 'Last Compiled `r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document:
    
    code_folding: show
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
               collapse = FALSE,
               comment = "",
               strip.white = TRUE,
               warning = FALSE,
               message = FALSE,
               cache = TRUE,
               out.width = "100%",
               fig.align = "center")

```

## load Packages
```{r}
library(data.table)
library(lubridate)
library(caret)
library(rai)
library(forecast)
library(parallel)
library(doParallel)
library(tidyverse)
library(gridExtra)
```

## Paths
```{r}
datapath = "../data/"
trends_path = paste0(datapath, "googleTrendsAuto/")
ind_path = paste0(datapath, "indicators/")
```

## Functions
```{r}
scale2 <- function(x, na.rm = TRUE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)
```


# Read data {.tabset .tabset-fade}
## Volatility Index {-}
```{r vol_ind}
vol_ind = read_delim(paste0(datapath,"country_volatility_index.csv")) %>%
  mutate(geo = str_split(Countries, " ", simplify = T)[,1], .before = 1,
         country = str_remove(Countries, ".*\\("), 
         country = str_extract(country, ".*(?=\\))"))

country_code_list = unlist(vol_ind$geo)
country_list = unlist(vol_ind$country)
country_code_list[country_list=="Greece"] <- "GR"
```

## Indicators {-}
Read indicator data and filter for valid countries. Countries are valid if they appear in the country volatility index.

```{r ind1}
list.files(ind_path)
ind_in = fread(paste0(ind_path,"tour_occ_nim_linear_all.csv"))

ind_t1 = ind_in[,-c(1:3)] %>% 
  as_tibble() %>%
  mutate(month_end = as_date(paste0(TIME_PERIOD,"-01")),.before=1) %>%
  filter(geo %in% vol_ind$geo)

ind_t1 %>% select(nace_r2) %>% distinct()
```

Build dataset:
```{r ind2}
df_ind = ind_t1 %>%
  filter(nace_r2 == "I551-I553" & unit=="NR") %>%
  group_by(geo, c_resid) %>%
  mutate(obs_value_std = scale2(OBS_VALUE), mean=mean(OBS_VALUE), sd = sd(OBS_VALUE)) %>%
  ungroup()

ind_wide = ind_t1 %>%
  filter(nace_r2 == "I551-I553" & unit=="NR") %>%
  mutate(obs_value_std = OBS_VALUE) %>%
  pivot_wider(id_cols = c(month_end, geo), names_from=c_resid, values_from = obs_value_std, names_prefix="resid_") %>%
  group_by(geo) %>%
  mutate(resid_DOM = coalesce(resid_DOM, resid_NAT),
         share_FOR = resid_FOR / resid_TOTAL,
         month = month(month_end),
         year = year(month_end),
         resid_DOM_STD = scale2(resid_DOM),
         resid_FOR_STD = scale2(resid_FOR),
         resid_TOTAL_STD = scale2(resid_TOTAL),
         ) %>%
  select(!resid_NAT) %>%
  ungroup() %>%
  arrange(geo,month_end)

ind_wide %>%
  filter(is.na(resid_DOM)) %>%
  distinct(geo)

ind_stats = ind_wide %>%
  group_by(geo) %>%
  summarise(resid_TOTAL_mean=mean(resid_TOTAL, na.rm = T), 
            resid_TOTAL_sd=sd(resid_TOTAL, na.rm = T), 
            resid_TOTAL_max=max(resid_TOTAL, na.rm = T), 
            resid_TOTAL_norm=max(abs(resid_TOTAL-mean(resid_TOTAL, na.rm = T)), na.rm=T)
            )

geo_minmax = ind_wide %>%
  group_by(geo) %>%
  summarise(min = min(year), max = max(year))

```

## Google Trends {-}
```{r trends}
list.files(trends_path)
goo = read_csv(paste0(trends_path, "trends_full.csv"))
trends_100 = goo %>%
  mutate(across(3:13, ~.x/100))
trends_norm = goo %>%
  mutate(across(3:13, scale2))
```

## Combine data {-}
```{r combine}
trends = ind_wide %>%
  select(month_end, geo, resid_TOTAL_STD) %>%
  rename(date = month_end, country_code = geo) %>%
  inner_join(trends_100, by=c("date", "country_code"))
```

# Plots {.tabset .tabset-fade}
```{r}
colnames(trends)
```

## country_trav {-}
```{r plot1}
trends %>%
  ggplot(mapping=aes(x=date, y=country_trav, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```


## country_cc_trav {-}
```{r plot2}
trends %>%
  ggplot(mapping=aes(x=date, y=country_cc_trav, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```

## country_hotels {-}
```{r plot3}
trends %>%
  ggplot(mapping=aes(x=date, y=country_hotels, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```

## cc_restaurant {-}
```{r plot4}
trends %>%
  ggplot(mapping=aes(x=date, y=cc_restaurant, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```

## cc_fooddrink {-}
```{r plot5}
trends %>%
  ggplot(mapping=aes(x=date, y=cc_fooddrink, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```

# Modelling country_wise Italy {.tabset .tabset-fade}
## Set up data {-}
```{r model data}
i=13

c_code = country_code_list[i]
ctry = country_list[i]

trends_cty = trends %>%
  filter(country_code == c_code)

trends_ts = trends_cty %>%
  select(!c(country_code,date)) %>%
  ts(start=c(2004,1), frequency=12)

theData = trends_cty[,-c(1:3)]
theResponse = trends_cty$resid_TOTAL_STD

stats_list = list()
```

Enable Parallel  processing:
```{r}
cl = detectCores()-1
registerDoParallel(cl)
```


## RAI {-}
```{r  rai}
mod_rai = rai(theData = theData, theResponse = theResponse)
(rai_sum = summary(mod_rai$model))

sig_sq = (rai_sum$sigma)^2

stats_list[[1]] = data.frame(country = country_list[i],
                  TrainRMSE = RMSE(predict(mod_rai, theData),theResponse),
                  TrainRsquared  = rai_sum$r.squared,
                  TrainMAE = MAE(predict(mod_rai, theData),theResponse),
                  method = "RAI")
```


## arimax {-}
```{r arimax}
X = as.matrix(theData)
mod1 = auto.arima(y = theResponse, max.order = 12, xreg = X,  max.D = 2)
(ar_sum  = summary(mod1))

stats_list[[2]] = data.frame(country = country_list[i],
                  TrainRMSE = RMSE(ar_sum$fitted, ar_sum$x),
                  TrainRsquared  = NA,
                  TrainMAE = MAE(ar_sum$fitted, ar_sum$x),
                  method = "ARIMAX")
```

## SVM {-}
```{r svm}
fitControl <- caret::trainControl(method = "repeatedcv",
                            number = 15, ## 5-fold CV...
                            repeats = 15)  ## repeated 5 times
mod_svn =caret::train(x=theData, y=theResponse, method="svmRadialSigma", trControl = fitControl, verbose=F)
mod_svn
stats_list[[3]] = getTrainPerf(mod_svn) %>% 
  add_column(country = country_list[i], .before = 1)
```

## GBM {-}
```{r gbm}
gbmGrid <-  expand.grid(interaction.depth = c(1:4), 
                        n.trees = c(50,75,100, 150, 200), 
                        shrinkage = 0.1,
                        n.minobsinnode = 5)
mod_gbm =caret::train(x=theData, y=theResponse, method="gbm", trControl = fitControl, tuneGrid=gbmGrid, verbose=F)
mod_gbm

stats_list[[4]] = getTrainPerf(mod_gbm) %>% 
  add_column(country = country_list[i], .before = 1)
```

## RF {-}
```{r rf}
mod_rf =caret::train(x=theData, y=theResponse, method="rf", trControl = fitControl, verbose=F)
mod_rf


stats_list[[5]] = getTrainPerf(mod_rf) %>% 
  add_column(country = country_list[i], .before = 1)
```

## Bayesian Regression NN {-}
```{r}
theData = as.data.frame(theData)
fitControl <- caret::trainControl(method = "repeatedcv",
                            number = 15, ## 5-fold CV...
                            repeats = 15)  ## repeated 5 times
mod_brnn =caret::train(x=theData, y=theResponse, method="brnn", trControl = fitControl, verbose=F)
stats_list[[6]] = getTrainPerf(mod_rf) %>% 
  add_column(country = country_list[i], .before = 1)
```



# Results

```{r}
stopImplicitCluster()
res_df = do.call(rbind, stats_list)
res_df
```



