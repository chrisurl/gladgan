---
title: "import"
output: html_document
date: "2022-09-26"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message=FALSE)
```

## load Packages
```{r}
library(data.table)
library(tidyverse)
library(lubridate)
library(rai)
library(caret)
```

# Import data
## Thoughts
Do we need a lag? Meaning: are google trends coming into account later (for sure, but whats the timespan)? Can we measure this? If so, how?

## Assignment
Number of nights spent at tourist accommodation establishments (Eurobase code: TOUR_OCC_NIM, UNIT:NR, NACE R2: I551-I553, INDIC TO: B006)

In case there is no available reference to INDIC TO: B006 in the data, use the following setting: TOUR_OCC_NIM, UNIT:NR, NACE R2:I1551-I1553, TOTAL.

The published figures are available here - and the task of each team is to nowcast the values of this monthly indicator for September 2022 until April 2023.

In this phase, each team submits nowcasts (according to the approaches that they have developed) for the number of tourism nights spent at tourist accommodation establishments (tourism) indicator. The team can make up to 5 different entries, using different approaches. For each entry, nowcasts (in the form of point estimates) must be submitted for *at least 5 countries* (chosen by the team) for a given month. (For example, if the current reference period is September 2022, the teams will have to provide the point estimates for 5 or more countries to have a valid entry for September 2022.) For an entry to be considered in the evaluation phase, there must be *at least 6 consecutive monthly submissions* for each of at least 5 countries. Each monthly submission of an entry needs to be accompanied by a brief description of the approach used.

## Indicators
```{r}
datapath = "../data/"
ind_path = paste0(datapath, "indicators/")
list.files(ind_path)
ind_in = fread(paste0(ind_path,"tour_occ_nim_custom.csv"))

ind_t1 = ind_in[,-c(1:3)] %>% 
  as_tibble() %>%
  mutate(month_end = as_date(paste0(TIME_PERIOD,"-01")),.before=1)

ind_t1 %>% select(nace_r2) %>% distinct()

## Scale
scale2 <- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)

ind_de= ind_t1 %>% 
  filter(geo == "ES" & nace_r2 == "I551-I553" & unit=="NR" & c_resid=="TOTAL") %>%
  mutate(obs_value_std = scale2(OBS_VALUE),
         obs_value_norm = (OBS_VALUE-mean(OBS_VALUE))/max(abs(OBS_VALUE-mean(OBS_VALUE))),
         obs_value_max = OBS_VALUE/max(OBS_VALUE)
         )

ind_stats = ind_de %>%
  summarise(mean=mean(OBS_VALUE), 
            sd=sd(OBS_VALUE), 
            max=max(OBS_VALUE), 
            norm=max(abs(OBS_VALUE-mean(OBS_VALUE)))
            )
```

Therefore, the stats are:
```{r}
ind_stats
```


## Google Trends
```{r}
goo_path = paste0(datapath, "googleTrends/")
list.files(goo_path)
series = c("europa_de_week.csv", "europa_world_week.csv", "Germany_germany_weekly.csv", "Germany_worldwide_weekly.csv", "SearchGermany1.csv")
input = paste0(goo_path, series)
in_list =input %>% map(~fread(., skip=2))
in_list = map(in_list, as_tibble)

df = reduce(.x = in_list, .f = full_join)
div_100 = function(x) (x/100)
df = df %>% mutate(across(.cols = 2:6, scale2))

df_cl = df %>% 
   rename(eur_world=`europa: (Weltweit)`,
          eur_de=`europa: (Deutschland)`,
          de_de=`deutschland: (Deutschland)`, 
          de_world=`deutschland: (Weltweit)`, 
          tourism_de=`tourism: (Deutschland)`
          ) %>%
  mutate(week_end = Woche + 7, .before=2) %>%
  mutate(month_end = floor_date(week_end, unit="month"), .before=3)

month_cnt = df_cl %>% select(month_end) %>% group_by(month_end) %>% summarise(n=n())
ggplot(data=month_cnt, aes(x=month_end, y=n)) + 
  geom_col()
```

For now, this looks fine.
To do if accuracy needs a boost: Divide overlapping weeks proportional into both neighbouring regions.

Now, use mean to aggregate the weekly to monthly data. Means because we have trend data which represents an importance rating between 0 and 1, so a sum would be completely wrong.
```{r}
df_cl_month = df_cl %>% 
  select(!week_end) %>%
  group_by(month_end) %>%
  summarise(across(.cols = 2:6, mean))
            
df_cl_month %>%
  gather(key=series, value=value,-month_end) %>%
  ggplot(aes(x=month_end, y=value, color=series)) + 
  geom_line()
```

# Combine data

```{r}
df_de = ind_de[,-c(2:6,8)] %>%
  full_join(df_cl_month, by="month_end") %>%
  filter(month_end > as_date("2017-09-01")) %>%
  select(!tourism_de)

df_de %>% 
  filter_all(any_vars(is.na(.)))

theResponse=df_de$obs_value_std[-c(58:60)]
theData = df_de[-c(58:60),6:9]
```

# Test

## LM
```{r}
lm1 = lm(obs_value_std  ~ eur_de+eur_world+de_de+de_world, data=df_de)
summary(lm1)
mod_lm =caret::train(x=theData, y=theResponse, method="lm")

lm2 = lm(obs_value_std  ~ eur_de+eur_world+de_de, data=df_de)
summary(lm2)
```

We do need more data! Seems that google trends monthly data should be sufficient!

## RAI
```{r}
rai_out = rai(theData, theResponse)
summary(rai_out$model)
```

## Bayesian reg NN
```{r brnn, cache=T, message=F, warning=F}

theData = as.data.frame(theData)
fitControl <- caret::trainControl(method = "repeatedcv",
                            number = 5, ## 5-fold CV...
                            repeats = 5)  ## repeated 5 times
mod_brnn =caret::train(x=theData, y=theResponse, method="brnn", trControl = fitControl, verbose=F)
```

```{r}
mod_brnn
predict(mod_brnn, df_de[c(58:60),6:9])
```

## Deep Autoencoder (not working)
Something is wrong; all the RMSE metric values are missing:
```{r,  eval=F}
fitControl <- caret::trainControl(method = "repeatedcv",
                            number = 5, ## 5-fold CV...
                            repeats = 5)  ## repeated 5 times
mod_nn =caret::train(x=theData, y=theResponse, method="dnn", trControl = fitControl, verbose=F)
```

```{r, eval=F}
mod_nn
predict(mod_nn, df_de[c(58:60),6:9])
```

## GBM

```{r gbm1, cache=T, message=F, warning=F}
gbmGrid <-  expand.grid(interaction.depth = c(1:4), 
                        n.trees = (1:50), 
                        shrinkage = 0.1,
                        n.minobsinnode = 5)

fitControl <- caret::trainControl(method = "repeatedcv",
                            number = 5, ## 5-fold CV...
                            repeats = 10)  ## repeated 5 times
mod_gbm =caret::train(x=theData, y=theResponse, method="gbm", trControl = fitControl, tuneGrid=gbmGrid, verbose=F)
```

```{r}
#mod_gbm
mod_gbm$bestTune
mod_gbm$results %>% filter(n.trees==33 & interaction.depth==1 & shrinkage == 0.1 & n.minobsinnode==5)
summary(mod_gbm)
predict(mod_gbm, df_de[c(58:60),6:9])
```

## SVN

http://topepo.github.io/caret/train-models-by-tag.html#support-vector-machines

Die sehen sehr vielversprechend aus! Das sollte man weiter verfolgen.

```{r svn, cache=T, message=F, warning=F}
fitControl <- caret::trainControl(method = "repeatedcv",
                            number = 5, ## 5-fold CV...
                            repeats = 5)  ## repeated 5 times
mod_svn =caret::train(x=theData, y=theResponse, method="svmRadialSigma", trControl = fitControl, verbose=F)
```

```{r}
mod_svn
summary(mod_svn)
predict(mod_svn, df_de[c(58:60),6:9])
```

# Compare results:
```{r}
res_tab = tibble(
  lm = predict(mod_lm,df_de[c(58:60),6:9]),
  brnn = predict(mod_brnn, df_de[c(58:60),6:9]),
  gbm = predict(mod_gbm, df_de[c(58:60),6:9]),
  svn = predict(mod_svn, df_de[c(58:60),6:9])
)

res_stats = as_tibble(getTrainPerf(mod_lm)) %>%
  add_row(getTrainPerf(mod_brnn)) %>%
  add_row(getTrainPerf(mod_gbm)) %>%
  add_row(getTrainPerf(mod_svn))

res_tab
res_stats

summary(rai_out$model)
```

Last but not least: Variable importance:
```{r}
varImp(mod_svn)
```

Die Suchen nach Europa (Kategorie Reisen) in Deutschland und der Welt haben wohl den größten Einfluss für die Übernachtungen in Deutschland.