---
title: "Google trends models"
author: Christian Url
date: 'Last Compiled `r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document:
    
    code_folding: show
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
               collapse = FALSE,
               comment = "",
               strip.white = TRUE,
               warning = FALSE,
               message = FALSE,
               cache = TRUE,
               out.width = "100%",
               fig.align = "center")

```

## load Packages
```{r, cache=FALSE}
library(data.table)
library(lubridate)
library(caret)
library(rai)
library(forecast)
library(parallel)
library(doParallel)
library(tidyverse)
```

## Paths
```{r}
datapath = "../data/"
trends_path = paste0(datapath, "googleTrendsAuto/")
ind_path = paste0(datapath, "indicators/")
```

## Functions
```{r}
scale2 <- function(x, na.rm = TRUE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)
```


# Read data {.tabset .tabset-fade}
## Volatility Index {-}
```{r vol_ind}
vol_ind = read_delim(paste0(datapath,"country_volatility_index.csv")) %>%
  mutate(geo = str_split(Countries, " ", simplify = T)[,1], .before = 1,
         country = str_remove(Countries, ".*\\("), 
         country = str_extract(country, ".*(?=\\))"))

country_code_list = unlist(vol_ind$geo)
country_list = unlist(vol_ind$country)
#country_code_list[country_list=="Greece"] <- "GR"
```

## Indicators {-}
Read indicator data and filter for valid countries. Countries are valid if they appear in the country volatility index.

```{r ind1}
list.files(ind_path)
ind_in = fread(paste0(ind_path,"tour_occ_nim_linear_all.csv"))

ind_t1 = ind_in[,-c(1:3)] %>% 
  as_tibble() %>%
  mutate(month_end = as_date(paste0(TIME_PERIOD,"-01")),.before=1) %>%
  filter(geo %in% vol_ind$geo)

ind_t1 %>% select(nace_r2) %>% distinct()
```

Build dataset:
```{r ind2}
df_ind = ind_t1 %>%
  filter(nace_r2 == "I551-I553" & unit=="NR") %>%
  group_by(geo, c_resid) %>%
  mutate(obs_value_std = scale2(OBS_VALUE), mean=mean(OBS_VALUE), sd = sd(OBS_VALUE)) %>%
  ungroup()

ind_wide = ind_t1 %>%
  filter(nace_r2 == "I551-I553" & unit=="NR") %>%
  mutate(obs_value_std = OBS_VALUE) %>%
  pivot_wider(id_cols = c(month_end, geo), names_from=c_resid, values_from = obs_value_std, names_prefix="resid_") %>%
  group_by(geo) %>%
  mutate(resid_DOM = coalesce(resid_DOM, resid_NAT),
         share_FOR = resid_FOR / resid_TOTAL,
         month = month(month_end),
         year = year(month_end),
         resid_DOM_STD = scale2(resid_DOM),
         resid_FOR_STD = scale2(resid_FOR),
         resid_TOTAL_STD = scale2(resid_TOTAL),
         ) %>%
  select(!resid_NAT) %>%
  ungroup() %>%
  arrange(geo,month_end)

ind_wide %>%
  filter(is.na(resid_DOM)) %>%
  distinct(geo)

ind_stats = ind_wide %>%
  group_by(geo) %>%
  summarise(resid_TOTAL_mean=mean(resid_TOTAL, na.rm = T), 
            resid_TOTAL_sd=sd(resid_TOTAL, na.rm = T), 
            resid_TOTAL_max=max(resid_TOTAL, na.rm = T), 
            resid_TOTAL_norm=max(abs(resid_TOTAL-mean(resid_TOTAL, na.rm = T)), na.rm=T)
            )

geo_minmax = ind_wide %>%
  group_by(geo) %>%
  summarise(min = min(year), max = max(year))

```

## Google Trends {-}
```{r trends}
list.files(trends_path)
goo = read_csv(paste0(trends_path, "trends_full.csv"))
trends_100 = goo %>%
  mutate(across(3:13, ~.x/100))
trends_norm = goo %>%
  mutate(across(3:13, scale2))
```

## Combine data {-}
```{r combine}
trends = ind_wide %>%
  select(month_end, geo, resid_TOTAL_STD) %>%
  rename(date = month_end, country_code = geo) %>%
  inner_join(trends_100, by=c("date", "country_code"))
```

# Plots {.tabset .tabset-fade}
```{r}
colnames(trends)
```

## country_trav {-}
```{r plot1}
trends %>%
  ggplot(mapping=aes(x=date, y=country_trav, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```


## country_cc_trav {-}
```{r plot2}
trends %>%
  ggplot(mapping=aes(x=date, y=country_cc_trav, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```

## country_hotels {-}
```{r plot3}
trends %>%
  ggplot(mapping=aes(x=date, y=country_hotels, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```

## cc_restaurant {-}
```{r plot4}
trends %>%
  ggplot(mapping=aes(x=date, y=cc_restaurant, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```

## cc_fooddrink {-}
```{r plot5}
trends %>%
  ggplot(mapping=aes(x=date, y=cc_fooddrink, group=country_code)) + 
  geom_line() + 
  facet_wrap(~country_code, ncol=3)
```

# Modelling for all countries {.tabset .tabset-fade}
Not evaluated.
```{r model data, eval = F}
source("trends_models_all.R")
```



# Results for div100 regressors {.tabset .tabset-fade}

```{r}
knitr::opts_chunk$set(cache = FALSE)
```


```{r, cache=FALSE}
stats = readr::read_csv("stats_alltrends_100.csv")
a  = stats %>%
  filter(method == "ARIMAX")

print(a, n=nrow(a))
```


## Overview {-}
```{r, cache=FALSE}
print(stats, n=nrow(stats))
```

## Best fits RMSE {-}
```{r, cache=FALSE}
rmse = stats %>%
  filter(method != "ARIMAX") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            min_RMSE = min(TrainRMSE),
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRMSE==min_RMSE)
print(rmse, n=nrow(rmse))
```

## Best fits R2 {-}
```{r}
r2 = stats %>%
  filter(method != "ARIMAX") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRsquared == max_R2)

print(r2, n=nrow(r2))
```

## Best fits MAE {-}

```{r}
mae = stats %>%
  filter(method != "ARIMAX") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, TrainMAE,
            min_MAE = min(TrainMAE)) %>%
  filter(TrainMAE == min_MAE)
  
print(mae, n=nrow(mae))
```

# Results for normalised regressors {.tabset .tabset-fade}


```{r, cache=FALSE}
stats = readr::read_csv("stats_alltrends_norm.csv")
a  = stats %>%
  filter(method == "ARIMAX")

print(a, n=nrow(a))
```


## Overview {-}
```{r, cache=FALSE}
print(stats, n=nrow(stats))
```

## Best fits RMSE {-}
```{r, cache=FALSE}
rmse = stats %>%
  filter(method != "ARIMAX") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            min_RMSE = min(TrainRMSE),
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRMSE==min_RMSE)
print(rmse, n=nrow(rmse))
```

## Best fits R2 {-}
```{r}
r2 = stats %>%
  filter(method != "ARIMAX") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRsquared == max_R2)

print(r2, n=nrow(r2))
```

## Best fits MAE {-}

```{r}
mae = stats %>%
  filter(method != "ARIMAX") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, TrainMAE,
            min_MAE = min(TrainMAE)) %>%
  filter(TrainMAE == min_MAE)
  
print(mae, n=nrow(mae))
```


# Results for lagged div100 regressors {.tabset .tabset-fade}


```{r, cache=FALSE}
stats = readr::read_csv("stats_alltrends_100_lag.csv")
```


## Overview {-}
```{r, cache=FALSE}
print(stats, n=nrow(stats))
```

## RAI {-}
```{r}
a  = stats %>%
  filter(method == "RAI")

print(a, n=nrow(a))
```

## Best fits RMSE {-}
```{r, cache=FALSE}
rmse = stats %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            min_RMSE = min(TrainRMSE),
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRMSE==min_RMSE)
print(rmse, n=nrow(rmse))
```

## Best fits R2 {-}
```{r}
r2 = stats %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRsquared == max_R2)

print(r2, n=nrow(r2))
```

## Best fits MAE {-}

```{r}
mae = stats %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, TrainMAE,
            min_MAE = min(TrainMAE)) %>%
  filter(TrainMAE == min_MAE)
  
print(mae, n=nrow(mae))
```


# Results for lagged normalised regressors {.tabset .tabset-fade}


```{r, cache=FALSE}
stats = readr::read_csv("stats_alltrends_norm_lag.csv")
```


## Overview {-}
```{r, cache=FALSE}
print(stats, n=nrow(stats))
```

## RAI {-}
```{r}
a  = stats %>%
  filter(method == "RAI")

print(a, n=nrow(a))
```

## Best fits RMSE {-}
```{r, cache=FALSE}
rmse = stats %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            min_RMSE = min(TrainRMSE),
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRMSE==min_RMSE)
print(rmse, n=nrow(rmse))
```

## Best fits R2 {-}
```{r}
r2 = stats %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared,
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRsquared == max_R2)

print(r2, n=nrow(r2))
```

## Best fits MAE {-}

```{r}
mae = stats %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, TrainMAE,
            min_MAE = min(TrainMAE)) %>%
  filter(TrainMAE == min_MAE)
  
print(mae, n=nrow(mae))
```

# Determine the best fit for google trends {.tabset .tabset-fade}
Will export a file containing all information using R2 as criterion.

## Load data (again)
```{r}
in_list = list()
in_list[[1]] = readr::read_csv("stats_alltrends_100.csv") %>%
  add_column(transform = "100")
in_list[[2]] = readr::read_csv("stats_alltrends_100_lag.csv") %>%
  add_column(transform = "100_lag")
in_list[[3]] = readr::read_csv("stats_alltrends_norm.csv") %>%
  add_column(transform = "norm")
in_list[[4]] = readr::read_csv("stats_alltrends_norm_lag.csv") %>%
  add_column(transform = "norm_lag")

stats_df = do.call(bind_rows, in_list)
```

## without RAI {.tabset .tabset-fade}

### Best fits RMSE {-}
```{r, stats_df=FALSE}
rmse = stats_df %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, transform,
            min_RMSE = min(TrainRMSE),
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRMSE==min_RMSE)
print(rmse, n=nrow(rmse))
```

### Best fits R2 {-}
```{r}
r2 = stats_df %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, transform,
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRsquared == max_R2)

print(r2, n=nrow(r2))
```

Export:
```{r}
write_csv(r2, file=paste0(datapath,"best_fits.csv"))
```


### Best fits MAE {-}

```{r}
mae = stats_df %>%
  filter(method != "RAI") %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, TrainMAE, transform,
            min_MAE = min(TrainMAE)) %>%
  filter(TrainMAE == min_MAE)
  
print(mae, n=nrow(mae))
```


## add RAI {.tabset .tabset-fade}

### Best fits RMSE {-}
```{r, stats_df=FALSE}
rmse = stats_df %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, transform,
            min_RMSE = min(TrainRMSE),
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRMSE==min_RMSE)
print(rmse, n=nrow(rmse))
```

### Best fits R2 {-}
```{r}
r2 = stats_df %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, transform,
            max_R2 = max(TrainRsquared)) %>%
  filter(TrainRsquared == max_R2)

print(r2, n=nrow(r2))
```

Export:
```{r}
write_csv(r2, file=paste0(datapath,"best_fits_RAI.csv"))
```

### Best fits MAE {-}

```{r}
mae = stats_df %>%
  group_by(country) %>%
  summarise(method, TrainRMSE, TrainRsquared, TrainMAE, transform,
            min_MAE = min(TrainMAE)) %>%
  filter(TrainMAE == min_MAE)
  
print(mae, n=nrow(mae))
```


# ARIMAX Modelling

__See extra file!__

Idea: Test variables in a stepwise procedure s.t. only the relevant terms are included in final model
Also use the hints discussed here: https://math.unm.edu/~lil/Stat581/10-dynamic-regression.pdf and https://www.r-bloggers.com/2021/10/dynamic-regression-with-arima-errors-the-students-on-the-streets/

Main Ideas: 

  * Try Fourier part with $K \in 1,2,...,6,12$
  * Trend, Drift, Season in extra terms or are they already included?
  * The *fable* package (https://fable.tidyverts.org/) should be an easy way to tidy up the estimation process 
  * Should we use the RAI Package to identify the regression part?


Readings: 

  * ARIMA Chapter: https://otexts.com/fpp2/arima.html
  * Dynamic regression: https://otexts.com/fpp2/dynamic.html
  * Neural network models: https://otexts.com/fpp2/nnetar.html
  